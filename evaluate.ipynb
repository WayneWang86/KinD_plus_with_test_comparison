{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ad841-f21e-41f1-8bd7-4c8b2baa5e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RD-529\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\Users\\RD-529\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\convolutional.py:414: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  warnings.warn('`tf.layers.conv2d` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RD-529\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RD-529\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\normalization.py:307: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ./checkpoint/decom_model/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/decom_model/model.ckpt\n",
      "loaded ./checkpoint/illu_model/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/illu_model/model.ckpt\n",
      "loaded ./checkpoint/restoration_model/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/restoration_model/model.ckpt\n",
      "(183, 275, 3)\n",
      "(180, 272, 3)\n",
      "(183, 275, 3)\n",
      "(180, 272, 3)\n",
      "(183, 275, 3)\n",
      "(180, 272, 3)\n",
      "(373, 560, 3)\n",
      "(372, 560, 3)\n",
      "(2000, 3008, 3)\n",
      "(2000, 3008, 3)\n",
      "(534, 800, 3)\n",
      "(532, 800, 3)\n",
      "Start evalating!\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from model import *\n",
    "from glob import glob\n",
    "from skimage import color,filters\n",
    "import argparse\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "\n",
    "# parser.add_argument('--save_dir', dest='save_dir', default='./test_results/test', help='directory for testing outputs')\n",
    "# parser.add_argument('--test_dir', dest='test_dir', default='./test_images/', help='directory for testing inputs')\n",
    "\n",
    "parser.add_argument('--save_dir', dest='save_dir', default='./test_results/face/', help='directory for testing outputs')\n",
    "parser.add_argument('--test_dir', dest='test_dir', default='./Test/face/', help='directory for testing inputs')\n",
    "\n",
    "parser.add_argument('--adjustment', dest='adjustment', default=False, help='whether to adjust illumination')\n",
    "parser.add_argument('--ratio', dest='ratio', default=5.0, help='ratio for illumination adjustment')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    " \n",
    "sess = tf.compat.v1.Session()\n",
    "training = tf.compat.v1.placeholder_with_default(False, shape=(), name='training')\n",
    "input_decom = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 3], name='input_decom')\n",
    "input_low_r = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 3], name='input_low_r')\n",
    "input_low_i = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 1], name='input_low_i')\n",
    "input_high_r = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 3], name='input_high_r')\n",
    "input_high_i = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 1], name='input_high_i')\n",
    "input_low_i_ratio = tf.compat.v1.placeholder(tf.compat.v1.float32, [None, None, None, 1], name='input_low_i_ratio')\n",
    "\n",
    "[R_decom, I_decom] = DecomNet(input_decom)\n",
    "decom_output_R = R_decom\n",
    "decom_output_I = I_decom\n",
    "output_r = Restoration_net(input_low_r, input_low_i, training)\n",
    "output_i = Illumination_adjust_net(input_low_i, input_low_i_ratio)\n",
    "\n",
    "# load pretrained model\n",
    "var_Decom = [var for var in tf.compat.v1.trainable_variables() if 'DecomNet' in var.name]\n",
    "var_adjust = [var for var in tf.compat.v1.trainable_variables() if 'I_enhance_Net' in var.name]\n",
    "var_restoration = [var for var in tf.compat.v1.trainable_variables() if 'Denoise_Net' in var.name]\n",
    "g_list = tf.compat.v1.global_variables()\n",
    "bn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\n",
    "bn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\n",
    "var_restoration += bn_moving_vars\n",
    "\n",
    "saver_Decom = tf.compat.v1.train.Saver(var_list = var_Decom)\n",
    "saver_adjust = tf.compat.v1.train.Saver(var_list=var_adjust)\n",
    "saver_restoration = tf.compat.v1.train.Saver(var_list=var_restoration)\n",
    "\n",
    "decom_checkpoint_dir ='./checkpoint/decom_model/'\n",
    "ckpt_pre=tf.compat.v1.train.get_checkpoint_state(decom_checkpoint_dir)\n",
    "if ckpt_pre:\n",
    "    print('loaded '+ckpt_pre.model_checkpoint_path)\n",
    "    saver_Decom.restore(sess,ckpt_pre.model_checkpoint_path)\n",
    "else:\n",
    "    print('No decomnet checkpoint!')\n",
    "\n",
    "checkpoint_dir_adjust = './checkpoint/illu_model/'\n",
    "ckpt_adjust=tf.compat.v1.train.get_checkpoint_state(checkpoint_dir_adjust)\n",
    "if ckpt_adjust:\n",
    "    print('loaded '+ckpt_adjust.model_checkpoint_path)\n",
    "    saver_adjust.restore(sess,ckpt_adjust.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"No adjust pre model!\")\n",
    "\n",
    "checkpoint_dir_restoration = './checkpoint/restoration_model/'\n",
    "ckpt=tf.compat.v1.train.get_checkpoint_state(checkpoint_dir_restoration)\n",
    "if ckpt:\n",
    "    print('loaded '+ckpt.model_checkpoint_path)\n",
    "    saver_restoration.restore(sess,ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"No restoration pre model!\")\n",
    "\n",
    "###load eval data\n",
    "eval_low_data = []\n",
    "eval_img_name =[]\n",
    "eval_low_data_name = glob(args.test_dir+'*')\n",
    "eval_low_data_name.sort()\n",
    "for idx in range(len(eval_low_data_name)):\n",
    "    [_, name] = os.path.split(eval_low_data_name[idx])\n",
    "    suffix = name[name.find('.') + 1:]\n",
    "    name = name[:name.find('.')]\n",
    "    eval_img_name.append(name)\n",
    "    eval_low_im = load_images(eval_low_data_name[idx])\n",
    "    print(eval_low_im.shape)\n",
    "    h,w,c = eval_low_im.shape\n",
    "# the size of test image H and W need to be multiple of 4, if it is not a multiple of 4, we will discard some border pixels.  \n",
    "    h_tmp = h%4\n",
    "    w_tmp = w%4\n",
    "    eval_low_im_resize = eval_low_im[0:h-h_tmp, 0:w-w_tmp, :]\n",
    "    print(eval_low_im_resize.shape)\n",
    "    eval_low_data.append(eval_low_im_resize)\n",
    "\n",
    "sample_dir = args.save_dir \n",
    "if not os.path.isdir(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "print(\"Start evalating!\")\n",
    "start_time = time.time()\n",
    "for idx in range(len(eval_low_data)):\n",
    "    print(idx)\n",
    "    name = eval_img_name[idx]\n",
    "    input_low = eval_low_data[idx]\n",
    "    input_low_eval = np.expand_dims(input_low, axis=0)\n",
    "    h, w, _ = input_low.shape\n",
    "\n",
    "    decom_r_low, decom_i_low = sess.run([decom_output_R, decom_output_I], feed_dict={input_decom: input_low_eval})\n",
    "    restoration_r = sess.run(output_r, feed_dict={input_low_r: decom_r_low, input_low_i: decom_i_low, training: False})\n",
    "### change the ratio to get different exposure level, the value can be 0-5.0\n",
    "    ratio = float(args.ratio)\n",
    "    i_low_data_ratio = np.ones([h, w])*(ratio)\n",
    "    i_low_ratio_expand = np.expand_dims(i_low_data_ratio , axis =2)\n",
    "    i_low_ratio_expand2 = np.expand_dims(i_low_ratio_expand, axis=0)\n",
    "    adjust_i = sess.run(output_i, feed_dict={input_low_i: decom_i_low, input_low_i_ratio: i_low_ratio_expand2})\n",
    "\n",
    "#The restoration result can find more details from very dark regions, however, it will restore the very dark regions\n",
    "#with gray colors, we use the following operator to alleviate this weakness.  \n",
    "    decom_r_sq = np.squeeze(decom_r_low)\n",
    "    r_gray = color.rgb2gray(decom_r_sq)\n",
    "    r_gray_gaussion = filters.gaussian(r_gray, 3)\n",
    "    low_i =  np.minimum((r_gray_gaussion*2)**0.5,1)\n",
    "    low_i_expand_0 = np.expand_dims(low_i, axis = 0)\n",
    "    low_i_expand_3 = np.expand_dims(low_i_expand_0, axis = 3)\n",
    "    result_denoise = restoration_r*low_i_expand_3\n",
    "    fusion4 = result_denoise*adjust_i\n",
    "\n",
    "    if args.adjustment:\n",
    "        fusion = decom_i_low*input_low_eval + (1-decom_i_low)*fusion4\n",
    "    else: \n",
    "        fusion = decom_i_low*input_low_eval + (1-decom_i_low)*result_denoise\n",
    "    #fusion2 = decom_i_low*input_low_eval + (1-decom_i_low)*restoration_r\n",
    "    save_images(os.path.join(sample_dir, '%s_KinD_plus.png' % (name)), fusion)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27a81c-a860-4338-a8e4-ce52db2fd2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91ee64-f08b-43db-b2fd-f23e13eadc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
